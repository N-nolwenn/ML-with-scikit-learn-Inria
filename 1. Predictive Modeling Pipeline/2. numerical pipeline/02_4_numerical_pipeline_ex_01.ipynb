{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2adce2f",
   "metadata": {},
   "source": [
    "# 📝 Exercise M1.03\n",
    "\n",
    "The goal of this exercise is to compare the performance of our classifier in\n",
    "the previous notebook (roughly 81% accuracy with `LogisticRegression`) to some\n",
    "simple baseline classifiers. The simplest baseline classifier is one that\n",
    "always predicts the same class, irrespective of the input data.\n",
    "\n",
    "- What would be the score of a model that always predicts `' >50K'`?\n",
    "- What would be the score of a model that always predicts `' <=50K'`?\n",
    "- Is 81% or 82% accuracy a good score for this problem?\n",
    "\n",
    "Use a `DummyClassifier` and do a train-test split to evaluate its accuracy on\n",
    "the test set. This\n",
    "[link](https://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators)\n",
    "shows a few examples of how to evaluate the generalization performance of\n",
    "these baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a79a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/INRIA/scikit-learn-mooc/master/datasets/adult-census.csv\"\n",
    "adult_census = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d21ff",
   "metadata": {},
   "source": [
    "We first split our dataset to have the target separated from the data used to\n",
    "train our predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3689a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "target = adult_census[target_name]\n",
    "data = adult_census.drop(columns=target_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39944b96",
   "metadata": {},
   "source": [
    "We start by selecting only the numerical columns as seen in the previous\n",
    "notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31529f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [\"age\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "\n",
    "data_numeric = data[numerical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcba570",
   "metadata": {},
   "source": [
    "Split the data and target into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffae09a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Write your code here.\n",
    "data_train, data_test, target_train, target_test = train_test_split(data_numeric, target, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5380644",
   "metadata": {},
   "source": [
    "Use a `DummyClassifier` such that the resulting classifier always predict the\n",
    "class `' >50K'`. What is the accuracy score on the test set? Repeat the\n",
    "experiment by always predicting the class `' <=50K'`.\n",
    "\n",
    "Hint: you can set the `strategy` parameter of the `DummyClassifier` to achieve\n",
    "the desired behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bd7d579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m \u001b[0mDummyClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'prior'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "DummyClassifier makes predictions that ignore the input features.\n",
      "\n",
      "This classifier serves as a simple baseline to compare against other more\n",
      "complex classifiers.\n",
      "\n",
      "The specific behavior of the baseline is selected with the `strategy`\n",
      "parameter.\n",
      "\n",
      "All strategies make predictions that ignore the input feature values passed\n",
      "as the `X` argument to `fit` and `predict`. The predictions, however,\n",
      "typically depend on values observed in the `y` parameter passed to `fit`.\n",
      "\n",
      "Note that the \"stratified\" and \"uniform\" strategies lead to\n",
      "non-deterministic predictions that can be rendered deterministic by setting\n",
      "the `random_state` parameter if needed. The other strategies are naturally\n",
      "deterministic and, once fit, always return the same constant prediction\n",
      "for any value of `X`.\n",
      "\n",
      "Read more in the :ref:`User Guide <dummy_estimators>`.\n",
      "\n",
      ".. versionadded:: 0.13\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "strategy : {\"most_frequent\", \"prior\", \"stratified\", \"uniform\",             \"constant\"}, default=\"prior\"\n",
      "    Strategy to use to generate predictions.\n",
      "\n",
      "    * \"most_frequent\": the `predict` method always returns the most\n",
      "      frequent class label in the observed `y` argument passed to `fit`.\n",
      "      The `predict_proba` method returns the matching one-hot encoded\n",
      "      vector.\n",
      "    * \"prior\": the `predict` method always returns the most frequent\n",
      "      class label in the observed `y` argument passed to `fit` (like\n",
      "      \"most_frequent\"). ``predict_proba`` always returns the empirical\n",
      "      class distribution of `y` also known as the empirical class prior\n",
      "      distribution.\n",
      "    * \"stratified\": the `predict_proba` method randomly samples one-hot\n",
      "      vectors from a multinomial distribution parametrized by the empirical\n",
      "      class prior probabilities.\n",
      "      The `predict` method returns the class label which got probability\n",
      "      one in the one-hot vector of `predict_proba`.\n",
      "      Each sampled row of both methods is therefore independent and\n",
      "      identically distributed.\n",
      "    * \"uniform\": generates predictions uniformly at random from the list\n",
      "      of unique classes observed in `y`, i.e. each class has equal\n",
      "      probability.\n",
      "    * \"constant\": always predicts a constant label that is provided by\n",
      "      the user. This is useful for metrics that evaluate a non-majority\n",
      "      class.\n",
      "\n",
      "      .. versionchanged:: 0.24\n",
      "         The default value of `strategy` has changed to \"prior\" in version\n",
      "         0.24.\n",
      "\n",
      "random_state : int, RandomState instance or None, default=None\n",
      "    Controls the randomness to generate the predictions when\n",
      "    ``strategy='stratified'`` or ``strategy='uniform'``.\n",
      "    Pass an int for reproducible output across multiple function calls.\n",
      "    See :term:`Glossary <random_state>`.\n",
      "\n",
      "constant : int or str or array-like of shape (n_outputs,), default=None\n",
      "    The explicit constant as predicted by the \"constant\" strategy. This\n",
      "    parameter is useful only for the \"constant\" strategy.\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "classes_ : ndarray of shape (n_classes,) or list of such arrays\n",
      "    Unique class labels observed in `y`. For multi-output classification\n",
      "    problems, this attribute is a list of arrays as each output has an\n",
      "    independent set of possible classes.\n",
      "\n",
      "n_classes_ : int or list of int\n",
      "    Number of label for each output.\n",
      "\n",
      "class_prior_ : ndarray of shape (n_classes,) or list of such arrays\n",
      "    Frequency of each class observed in `y`. For multioutput classification\n",
      "    problems, this is computed independently for each output.\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X` has\n",
      "    feature names that are all strings.\n",
      "\n",
      "n_outputs_ : int\n",
      "    Number of outputs.\n",
      "\n",
      "sparse_output_ : bool\n",
      "    True if the array returned from predict is to be in sparse CSC format.\n",
      "    Is automatically set to True if the input `y` is passed in sparse\n",
      "    format.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "DummyRegressor : Regressor that makes predictions using simple rules.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> import numpy as np\n",
      ">>> from sklearn.dummy import DummyClassifier\n",
      ">>> X = np.array([-1, 1, 1, 1])\n",
      ">>> y = np.array([0, 1, 1, 1])\n",
      ">>> dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
      ">>> dummy_clf.fit(X, y)\n",
      "DummyClassifier(strategy='most_frequent')\n",
      ">>> dummy_clf.predict(X)\n",
      "array([1, 1, 1, 1])\n",
      ">>> dummy_clf.score(X, y)\n",
      "0.75\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\npigeon1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\sklearn\\dummy.py\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "DummyClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d79e888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' >50K' ' <=50K']\n",
      "<class 'pandas.core.series.Series'>\n",
      "[\"' >50K'\", \"' <=50K'\"]\n"
     ]
    }
   ],
   "source": [
    "print(target_train.unique())\n",
    "print(type(target_train))\n",
    "print([f\"'{x}'\" for x in target_train.unique()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "123283c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy using a DummyClassifier with constant =  >50K is 0.234\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "class_to_predict = ' >50K'\n",
    "high_revenue_clf = DummyClassifier(strategy = \"constant\", constant =class_to_predict)\n",
    "high_revenue_clf.fit(data_train, target_train)\n",
    "\n",
    "high_revenue_score = high_revenue_clf.score(data_test, target_test)\n",
    "\n",
    "dummy_name = high_revenue_clf.__class__.__name__\n",
    "dummy_strategy = high_revenue_clf.strategy\n",
    "strategy_constant = high_revenue_clf.constant\n",
    "\n",
    "print(f\"The test accuracy using a {dummy_name} with {dummy_strategy} = {strategy_constant} is {high_revenue_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1fe8972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy using a DummyClassifier with constant =  <=50K is 0.766\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "class_to_predict = ' <=50K'\n",
    "low_revenue_clf = DummyClassifier(strategy = \"constant\", constant =class_to_predict)\n",
    "low_revenue_clf.fit(data_train, target_train)\n",
    "\n",
    "low_revenue_score = low_revenue_clf.score(data_test, target_test)\n",
    "\n",
    "dummy_name = low_revenue_clf.__class__.__name__\n",
    "dummy_strategy = low_revenue_clf.strategy\n",
    "strategy_constant = low_revenue_clf.constant\n",
    "\n",
    "print(f\"The test accuracy using a {dummy_name} with {dummy_strategy} = {strategy_constant} is {low_revenue_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b440cb8e",
   "metadata": {},
   "source": [
    "We observe that this model has an accuracy higher than 0.5. This is due to the fact that we have 3/4 of the target belonging to low-revenue class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ce090d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "<=50K    37155\n",
       ">50K     11687\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_census[target_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d9e3906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7607182343065395)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(target == ' <=50K').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc8004",
   "metadata": {},
   "source": [
    "The `LogisticRegression` accuracy (roughly 81%) seems better than the `DummyClassifier` accuracy (roughly 76%). \n",
    "\n",
    "In a way, it is a bit reassuring, using a ML model gives a better performance than always predicting the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "24df0f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8070592089099992"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Write your code here.\n",
    "model = LogisticRegression()\n",
    "model.fit(data_train, target_train)\n",
    "predictions = model.predict(data_train)\n",
    "accuracy = model.score(data_test, target_test)\n",
    "accuracy\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "nbreset": "https://raw.githubusercontent.com/INRIA/scikit-learn-mooc/main/notebooks/02_numerical_pipeline_ex_01.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
